{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 000\n",
      "Processing folder: 001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bilal\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing audio\\001\\001486.mp3: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import concurrent.futures\n",
    "import warnings\n",
    "\n",
    "# Suppress PySoundFile and Audioread warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"PySoundFile failed. Trying audioread instead.\")\n",
    "\n",
    "def extract_features(audio_file, sr=22050, n_mfcc=13):\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(audio_file, sr=sr)\n",
    "        \n",
    "        # Extract MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        \n",
    "        # Compute mean of each MFCC coefficient\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        \n",
    "        return mfcc_mean\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process audio files in parallel\n",
    "def process_audio_files(audio_files):\n",
    "    features = []\n",
    "    for audio_file in audio_files:\n",
    "        mfcc_features = extract_features(audio_file)\n",
    "        if mfcc_features is not None:\n",
    "            file_name = os.path.basename(audio_file)  # Extract file name without path\n",
    "            features.append([file_name, *mfcc_features])\n",
    "    return features\n",
    "\n",
    "# Function to normalize features\n",
    "def normalize_features(features):\n",
    "    # Min-max normalization\n",
    "    normalized_features = (features - features.min()) / (features.max() - features.min())\n",
    "    return normalized_features\n",
    "\n",
    "# Function to standardize features\n",
    "def standardize_features(features):\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    standardized_features = scaler.fit_transform(features)\n",
    "    return standardized_features\n",
    "\n",
    "# Path to the folder containing subfolders of audio files\n",
    "fma_large_folder = \"audio\"\n",
    "\n",
    "# Initialize an empty list to store features\n",
    "features_list = []\n",
    "\n",
    "# Define the number of threads\n",
    "num_threads = os.cpu_count()\n",
    "\n",
    "# Loop through subfolders in fma_large\n",
    "for folder_name in sorted(os.listdir(fma_large_folder)):\n",
    "    folder_path = os.path.join(fma_large_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(\"Processing folder:\", folder_name)\n",
    "        \n",
    "        # List all MP3 files in the current subfolder\n",
    "        mp3_files = [os.path.join(folder_path, file_name) for file_name in sorted(os.listdir(folder_path)) if file_name.endswith(\".mp3\")]\n",
    "        \n",
    "        # Process audio files in parallel using multithreading\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            results = executor.map(process_audio_files, [mp3_files[i:i + num_threads] for i in range(0, len(mp3_files), num_threads)])\n",
    "        \n",
    "        # Flatten the list of features\n",
    "        for batch in results:\n",
    "            features_list.extend(batch)\n",
    "\n",
    "# Convert features list to DataFrame\n",
    "columns = ['filename'] + [f\"mfcc_{i}\" for i in range(len(features_list[0]) - 1)]\n",
    "features_df = pd.DataFrame(features_list, columns=columns)\n",
    "\n",
    "# Drop any rows with missing values (if any)\n",
    "features_df.dropna(inplace=True)\n",
    "\n",
    "# Separate the filename column\n",
    "file_names = features_df['filename']\n",
    "features_df.drop(columns=['filename'], inplace=True)\n",
    "\n",
    "# Apply normalization and standardization to features (excluding filename column)\n",
    "normalized_features = normalize_features(features_df)\n",
    "standardized_features = standardize_features(normalized_features)\n",
    "\n",
    "# Perform dimensionality reduction using PCA\n",
    "pca = PCA(n_components=10)  # Specify the number of components to keep\n",
    "reduced_features = pca.fit_transform(standardized_features)\n",
    "\n",
    "# Convert reduced features to DataFrame\n",
    "reduced_features_df = pd.DataFrame(reduced_features, columns=[f\"mfcc_{i}\" for i in range(reduced_features.shape[1])])\n",
    "\n",
    "# Concatenate filename column to reduced features DataFrame\n",
    "reduced_features_df.insert(0, 'filename', file_names)\n",
    "\n",
    "# Save the reduced features DataFrame to a new CSV file\n",
    "reduced_features_df.to_csv(\"audio_features1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        filename    mfcc_0    mfcc_1    mfcc_2    mfcc_3    mfcc_4    mfcc_5  \\\n",
      "0     000002.mp3 -0.213493 -0.889261  0.300114 -1.396363 -0.202126 -0.782602   \n",
      "1     000003.mp3  0.381634 -0.571500 -0.498392 -0.911056  0.144298 -0.135530   \n",
      "2     000005.mp3  1.516205 -0.187141  0.846744 -1.151170  0.056885 -1.181041   \n",
      "3     000010.mp3 -0.875783 -1.428132  0.076498  0.801129 -0.037355 -0.455636   \n",
      "4     000020.mp3  1.863987  0.421322 -0.032940 -0.049970  0.339167 -0.044251   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "1612  001995.mp3 -0.415149 -0.397714 -0.088524  0.741241 -2.236280  4.584156   \n",
      "1613  001996.mp3 -1.006368 -3.281262  0.643808  0.356302 -0.995869 -0.420052   \n",
      "1614  001997.mp3  0.666180  0.840533  0.375378 -0.177087 -0.941006 -0.638794   \n",
      "1615  001998.mp3 -0.478807  1.101714  0.993000 -0.420856 -0.027246  0.200849   \n",
      "1616  001999.mp3 -1.197814  2.253567  0.047800 -0.086981  0.215302 -0.164474   \n",
      "\n",
      "        mfcc_6    mfcc_7    mfcc_8    mfcc_9  \n",
      "0     0.427365 -0.378873 -0.206107  0.245426  \n",
      "1    -0.291108  0.238220 -0.190814  0.599711  \n",
      "2    -0.998594  0.692018 -0.193816 -0.372843  \n",
      "3     0.287732 -0.704623 -0.147388  0.342394  \n",
      "4     0.014663 -0.472649 -0.010678 -0.493336  \n",
      "...        ...       ...       ...       ...  \n",
      "1612  1.064373  0.191030  0.069648  1.085470  \n",
      "1613  0.003457  1.725087  0.414698  0.666150  \n",
      "1614 -0.147901  0.232281 -0.264232  0.748877  \n",
      "1615 -0.308247  0.669342  0.328826 -0.006171  \n",
      "1616  0.140568  0.568587 -0.054661  0.379451  \n",
      "\n",
      "[1617 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'path_to_your_csv_file' with the actual path to your CSV file\n",
    "csv_file_path = 'audio_features1.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Transformed data has been successfully stored in MongoDB.\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    # Load the reduced features from the CSV file\n",
    "    reduced_features_df = pd.read_csv('audio_features1.csv')\n",
    "\n",
    "    # MongoDB connection details\n",
    "    mongodb_url = \"mongodb://localhost:27017/\"\n",
    "    database_name = \"BDA\"\n",
    "    collection_name = \"audio_features\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    client = pymongo.MongoClient(mongodb_url)\n",
    "    db = client[database_name]\n",
    "\n",
    "    # Check if the connection to MongoDB is successful\n",
    "    if db is not None:\n",
    "        # Convert DataFrame to dictionary\n",
    "        transformed_data_dict = reduced_features_df.to_dict(orient='records')\n",
    "\n",
    "        # Insert data into MongoDB collection\n",
    "        collection = db[collection_name]\n",
    "        collection.insert_many(transformed_data_dict)\n",
    "\n",
    "        # Log success message\n",
    "        logger.info(\"Transformed data has been successfully stored in MongoDB.\")\n",
    "    else:\n",
    "        # Log error if connection to MongoDB fails\n",
    "        logger.error(\"Failed to connect to MongoDB.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log error if any exception occurs during processing\n",
    "    logger.error(f\"An error occurred during processing: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close MongoDB connection\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for 000002.mp3:\n",
      "1. 000002.mp3 - Similarity: 1.0000\n",
      "2. 001613.mp3 - Similarity: 0.4589\n",
      "3. 001329.mp3 - Similarity: 0.4474\n",
      "4. 001642.mp3 - Similarity: 0.4442\n",
      "5. 000587.mp3 - Similarity: 0.4318\n",
      "6. 001602.mp3 - Similarity: 0.4278\n",
      "7. 000744.mp3 - Similarity: 0.4225\n",
      "8. 001811.mp3 - Similarity: 0.4210\n",
      "9. 001334.mp3 - Similarity: 0.4200\n",
      "10. 001591.mp3 - Similarity: 0.4191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "# Load the reduced features from the CSV file\n",
    "reduced_features_df = pd.read_csv('audio_features1.csv')\n",
    "\n",
    "# Separate the filename column\n",
    "filenames = reduced_features_df['filename']\n",
    "features = reduced_features_df.drop(columns=['filename'])\n",
    "\n",
    "# Initialize ANN model\n",
    "n_neighbors = 10  # Number of neighbors to consider\n",
    "ann_model = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto', metric='euclidean')\n",
    "\n",
    "# Fit the model with the features\n",
    "ann_model.fit(features)\n",
    "\n",
    "# Example usage: find nearest neighbors for a sample audio file\n",
    "sample_index = 0  # Index of the sample audio file\n",
    "sample_features = features.iloc[[sample_index]]\n",
    "distances, indices = ann_model.kneighbors(sample_features)\n",
    "\n",
    "# Get the filenames of nearest neighbors\n",
    "nearest_neighbor_filenames = filenames.iloc[indices[0]].tolist()\n",
    "print(f\"Nearest neighbors for {filenames.iloc[sample_index]}:\")\n",
    "for i, (filename, distance) in enumerate(zip(nearest_neighbor_filenames, distances[0]), 1):\n",
    "    similarity = 1 / (1 + distance)  # Calculate similarity\n",
    "    print(f\"{i}. {filename} - Similarity: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 517.4972, Val Loss: 481.5277\n",
      "Epoch [2/10], Train Loss: 497.0131, Val Loss: 462.9224\n",
      "Epoch [3/10], Train Loss: 478.0166, Val Loss: 445.6771\n",
      "Epoch [4/10], Train Loss: 460.3732, Val Loss: 429.6721\n",
      "Epoch [5/10], Train Loss: 443.9369, Val Loss: 414.7772\n",
      "Epoch [6/10], Train Loss: 428.5960, Val Loss: 400.8800\n",
      "Epoch [7/10], Train Loss: 414.2455, Val Loss: 387.8878\n",
      "Epoch [8/10], Train Loss: 400.7985, Val Loss: 375.7272\n",
      "Epoch [9/10], Train Loss: 388.1788, Val Loss: 364.3147\n",
      "Epoch [10/10], Train Loss: 376.3098, Val Loss: 353.5852\n",
      "Test Loss: 403.0534\n",
      "Recommendations:\n",
      "001290.mp3: Similarity - 0.8596255019266058\n",
      "000758.mp3: Similarity - 0.8478988081782778\n",
      "000744.mp3: Similarity - 0.8424415221250918\n",
      "001602.mp3: Similarity - 0.8380635628983775\n",
      "001613.mp3: Similarity - 0.7899577043613131\n",
      "001591.mp3: Similarity - 0.7808168897203976\n",
      "001642.mp3: Similarity - 0.7701039698132147\n",
      "001329.mp3: Similarity - 0.768673785739138\n",
      "001435.mp3: Similarity - 0.7507669721033767\n",
      "000587.mp3: Similarity - 0.7498322572318036\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit\n",
    "\n",
    "# Load the reduced features from the CSV file\n",
    "features_df = pd.read_csv('audio_features1.csv')\n",
    "\n",
    "# Separate the filename column\n",
    "filenames = features_df['filename']\n",
    "features = features_df.drop(columns=['filename'])\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_test = train_test_split(scaled_features, test_size=0.2, random_state=42)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = input_size  # Output size should match input size for autoencoder\n",
    "\n",
    "# Initialize model parameters\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = init_model_params(input_size, hidden_size, output_size, key)\n",
    "\n",
    "# Training loop with early stopping based on validation loss\n",
    "best_val_loss = float('inf')\n",
    "best_params = None\n",
    "patience = 5\n",
    "for epoch in range(num_epochs):\n",
    "    grads = grad_loss(params, X_train, X_train)\n",
    "    params = [w - learning_rate * dw for w, dw in zip(params, grads)]\n",
    "    \n",
    "    # Calculate training loss\n",
    "    train_loss = loss(params, X_train, X_train)\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    val_loss = loss(params, X_val, X_val)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_params = params\n",
    "        patience = 5\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "# Evaluate model performance on the test set\n",
    "test_loss = loss(best_params, X_test, X_test)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Calculate similarities with the selected audio\n",
    "selected_features = scaled_features[selected_index]\n",
    "similarities = cosine_similarity([selected_features], scaled_features)[0]\n",
    "\n",
    "# Get indices of top 10 similar audio files\n",
    "top_indices = np.argsort(similarities)[-11:-1][::-1]  # Top 10 indices excluding the selected audio itself\n",
    "\n",
    "# Print recommendations\n",
    "print(\"Recommendations:\")\n",
    "for idx in top_indices:\n",
    "    print(f\"{filenames.iloc[idx]}: Similarity - {similarities[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data from CSV file\n",
    "df = pd.read_csv('audio_features1.csv')\n",
    "\n",
    "# Calculate the average value of MFCC features for each file\n",
    "df['target'] = df.drop(['filename'], axis=1).mean(axis=1)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop(['filename', 'target'], axis=1)  # features\n",
    "y = df['target']  # target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a random forest regressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "\n",
    "# Save the trained model to a pickle file\n",
    "with open('recommendation_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn's classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bilal\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bilal\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bilal\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv('audio_features1.csv')\n",
    "\n",
    "# Extract true labels from filenames\n",
    "true_labels = [1 if filename.startswith('positive') else 0 for filename in data['filename']]\n",
    "\n",
    "# Separate the filename column\n",
    "features = data.drop(columns=['filename'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, true_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train recommendation model using scikit-learn (Nearest Neighbors)\n",
    "n_neighbors = 10\n",
    "model = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto', metric='euclidean')\n",
    "model.fit(X_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "distances, indices = model.kneighbors(X_test)\n",
    "\n",
    "# Generate predicted labels based on distances to nearest neighbors\n",
    "predicted_labels = [1 if any(y_train[neighbor] == 1 for neighbor in neighbors) else 0 for neighbors in indices]\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, predicted_labels)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, predicted_labels)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0\n",
      "Recall: 0\n",
      "F1-score: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv('audio_features1.csv')\n",
    "\n",
    "# Separate the filename column\n",
    "filenames = data['filename']\n",
    "features = data.drop(columns=['filename'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test = train_test_split(features, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train recommendation model using scikit-learn (Nearest Neighbors)\n",
    "n_neighbors = 10\n",
    "model = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto', metric='euclidean')\n",
    "model.fit(X_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "distances, indices = model.kneighbors(X_test)\n",
    "\n",
    "# Assuming you have true labels and predicted labels for your recommendation model\n",
    "# Here, we'll generate random true labels for demonstration purposes\n",
    "true_labels = [1 if filename.startswith('positive') else 0 for filename in filenames]\n",
    "\n",
    "# Generate predicted labels based on distances to nearest neighbors\n",
    "predicted_labels = [1 if any(true_labels[neighbor] for neighbor in neighbors) else 0 for neighbors in indices]\n",
    "\n",
    "# Calculate true positives, false positives, and false negatives\n",
    "tp = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == 1 and pred == 1)\n",
    "fp = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == 0 and pred == 1)\n",
    "fn = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == 1 and pred == 0)\n",
    "\n",
    "# Calculate precision only if there are positive predictions\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "# Calculate recall only if there are positive instances in the data\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0\n",
      "Recall: 0\n",
      "F1-score: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv('audio_features1.csv')\n",
    "\n",
    "# Separate the filename column\n",
    "filenames = data['filename']\n",
    "features = data.drop(columns=['filename'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train recommendation model using scikit-learn (Nearest Neighbors)\n",
    "n_neighbors = 10\n",
    "model = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto', metric='euclidean')\n",
    "model.fit(X_train.drop(columns=['filename']))  # Dropping filename for training\n",
    "\n",
    "# Make predictions on the test set\n",
    "distances, indices = model.kneighbors(X_test.drop(columns=['filename']))  # Dropping filename for prediction\n",
    "\n",
    "# Generate true labels based on filenames\n",
    "true_labels = [1 if 'positive' in filename else 0 for filename in X_test['filename']]\n",
    "\n",
    "# Generate predicted labels based on distances to nearest neighbors\n",
    "predicted_labels = []\n",
    "\n",
    "for neighbors in indices:\n",
    "    positive_neighbor = False\n",
    "    for neighbor in neighbors:\n",
    "        if 'positive' in X_train.iloc[neighbor]['filename']:\n",
    "            positive_neighbor = True\n",
    "            break\n",
    "    if positive_neighbor:\n",
    "        predicted_labels.append(1)\n",
    "    else:\n",
    "        predicted_labels.append(0)\n",
    "\n",
    "# Calculate true positives, false positives, and false negatives\n",
    "tp = np.sum(np.logical_and(true_labels == 1, np.array(predicted_labels) == 1))\n",
    "fp = np.sum(np.logical_and(true_labels == 0, np.array(predicted_labels) == 1))\n",
    "fn = np.sum(np.logical_and(true_labels == 1, np.array(predicted_labels) == 0))\n",
    "\n",
    "# Calculate precision\n",
    "if tp + fp == 0:\n",
    "    precision = 0  # Handle the case where there are no positive predictions\n",
    "else:\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "# Calculate recall\n",
    "if tp + fn == 0:\n",
    "    recall = 0  # Handle the case where there are no positive instances\n",
    "else:\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "# Calculate F1-score\n",
    "if precision + recall == 0:\n",
    "    f1 = 0  # Handle the case where both precision and recall are zero\n",
    "else:\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
